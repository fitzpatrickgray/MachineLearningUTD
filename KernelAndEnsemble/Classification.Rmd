---
title: "SVM Classification"
author: "Spencer Gray"
date: "2022-10-23"
output:
  html_document: default
  pdf_document: default
---


## Data Setup

In this notebook we will try and guess whether the observed data corresponds to a star, galaxy or a quasar. Training the SVM model requires a value target, so we define a factor to each classification. For this example I will assign 0 to star, 1 to galaxy, 2 to quasar. This dataset is also quite large at 100,000 samples so I am going to limit its size to 15,00 with 12,000 in the training set and 3,000 in the test.

```{r}
library(dplyr)
set.seed(1234)

star <- read.csv("star_classification.csv")

star$class[which(star$class=="STAR")] <- 0
star$class[which(star$class=="GALAXY")] <- 1
star$class[which(star$class=="QSO")] <- 2
star$class <- as.factor(star$class)

train <- sample_n(star, 12000)
test <- sample_n(star, 3000)


dim(star)
head(star)
```

## Graphing Color Channels

Boxplot of UV, Green, Red, Near IR, and IR channels of quasars, stars. We could be seeing a more tight box with Quasars due to their rarity (less samples) relative to galaxies and stars.
```{r}
par(mfrow=c(1,3))

boxplot(star$u[star$class == 0], star$g[star$class == 0], star$r[star$class == 0], star$i[star$class == 0], star$z[star$class == 0], names=c("UV", "Green", "Red", "Near IR", "IR"), ylim= c(10, 32), ylab=("Value"), xlab=("Star"), col=c("purple", "green", "red", "darkred", "white"))

boxplot(star$u[star$class == 1], star$g[star$class == 1], star$r[star$class == 1], star$i[star$class == 1], star$z[star$class == 1], names=c("UV", "Green", "Red", "Near IR", "IR"), ylim= c(10, 32), ylab=("Value"), xlab=("Galaxy"), col=c("purple", "green", "red", "darkred", "white"))

boxplot(star$u[star$class == 2], star$g[star$class == 2], star$r[star$class == 2], star$i[star$class == 2], star$z[star$class == 2], names=c("UV", "Green", "Red", "Near IR", "IR"), ylim= c(10, 32), ylab=("Value"), xlab=("Quasar"), col=c("purple", "green", "red", "darkred", "white"))

```

## Graphing Alpha
```{r}
par(mfrow=c(1,3))
boxplot(star$alpha[star$class == 0], names=c("Alpha"), ylim= c(0, 400), ylab=("Value"), xlab=("Star"), col=("lightblue"))
boxplot(star$alpha[star$class == 1], names=c("Alpha"), ylim= c(0, 400), ylab=("Value"), xlab=("Galaxy"), col=("lightblue"))
boxplot(star$alpha[star$class == 2], names=c("Alpha"), ylim= c(0, 400), ylab=("Value"), xlab=("Quasar"), col=("lightblue"))
```

## Graphing Delta
```{r}
par(mfrow=c(1,3))
boxplot(star$delta[star$class == 0], names=c("Delta"), ylim= c(-20, 85), ylab=("Value"), xlab=("Star"), col=("white"))
boxplot(star$delta[star$class == 1], names=c("Delta"), ylim= c(-20, 85), ylab=("Value"), xlab=("Galaxy"), col=("white"))
boxplot(star$delta[star$class == 2], names=c("Delta"), ylim= c(-20, 85), ylab=("Value"), xlab=("Quasar"), col=("white"))
```
## 3D Graph of Red, Green, IR of Each Subject
```{r}
library(rgl)
options(rgl.useNULL=TRUE)
colors <- c('darkcyan', 'purple', 'royalblue1')

plot3d(
  x=train$'u', y=train$'g', z=train$'i',
  col = colors[as.numeric(train$class)],
  type='s',
  radius=0.1,
  xlab="Ultraviolet", ylab="Green", zlab="Infrared"
)
rglwidget()
```

## SVM Training
```{r}
library(e1071)
set.seed(1234)

starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="linear", cost=1, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
```

## Tuning SVM
```{r}
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="linear", cost=1, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="polynomial", cost=1, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="radial", cost=1, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
```
Radial had the best results at 83.5% accuracy, so we use use it in our final model.

```{r}
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="radial", cost=0.001, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="radial", cost=0.01, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="radial", cost=0.1, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="radial", cost=1, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="radial", cost=10, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="radial", cost=100, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
```

Tuning shows us that the performance keeps on significantly improving but for the sake of not having insane training times we are going to stick with 10 despite the 0.8% gain from going 10 to 100.

```{r}
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="radial", cost=10, gamma=0.001, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="radial", cost=10, gamma=0.01, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="radial", cost=10, gamma=0.1, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="radial", cost=10, gamma=1, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="radial", cost=10, gamma=5, scale = TRUE)
pred <- predict(starSVM, newdata=test)
mean(pred==test$class)
```
Gamma of 1 appears to have the best results.


## Final Model
Calculating the correlation, mean squared error and the table of our svm model.
```{r}
starSVM <- svm(class ~ alpha + delta + u + g + r + i + z, data=train, kernel="radial", cost=10, gamma=1, scale = TRUE)
pred <- predict(starSVM, newdata=test)
table(pred, test$class)
mean(pred==test$class)
```

## Conclusion

From these results we see that a radial kernel fits our data the strongest in addition with some fine tuning of the cost and the gamma. This tells us that the categories of our data, galaxy, star, and quasar are grouped in spheres with respect to their alpha, delta channels as well as their electromagnetic spectrogram.